{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data from the Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "with open('ressources/sources.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "def clean_document(document):\n",
    "    page_content = document.page_content\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    clean_text = ' '.join(soup.get_text().split())\n",
    "    return clean_text\n",
    "\n",
    "# Extract the 'urls' list from the configuration\n",
    "urls = config.get('urls', [])\n",
    "loader = WebBaseLoader(urls)\n",
    "\n",
    "data = loader.load()\n",
    "for doc in data:\n",
    "    cleaned_text = clean_document(doc)\n",
    "    doc.page_content = cleaned_text\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_dir = os.getcwd()+\"\\docs\\Chatverlauf\\\\\"\n",
    "\n",
    "pdf_files = []\n",
    "\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_dir, filename)\n",
    "        pdf_files.append(file_path)\n",
    "\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the loaded Text into Chunks. Then use Chroma to vectorestore all the documents via openai. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ressources import config\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "data_splits = text_splitter.split_documents(data)\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "api_key = config.OPENAI_API_KEY\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=data_splits, embedding=openai_embeddings)\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    page = (loader.load())\n",
    "    page_splits = text_splitter.split_documents(page)\n",
    "    vectorstore.add_documents(page_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Vector Store as a retriever. If we now pass a Question to the OpenAI API the vectorstore makes sure to pass the context that fits the question most based on the vectors. We also pass the memory so the model \"remembers\" the entire conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "openai_api_key = api_key\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa(\"Welche verschiedenen Studeng√§nge gibt es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
